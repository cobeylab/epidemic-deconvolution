---
title: "Richardson-Lucy Blind Deconvolution"
output: html_notebook
---

RL deconvolution analysis by Lauren McGough, Simulations adapted from Katie Gostic

Last updated: 7-9-2020

Using blind deconvolution to compensate for a misspecified delay distribution. 
Specifically, using an iterative modification of RL developed for image processing when the blur kernel is not known precisely, alternating between holding the delay distribution constant and using RL to infer the blurred image, then holding the image constant and using RL to infer the delay distribution. 

Note: A WORK IN PROGRESS. This code runs, but is not working very well for inferring misspecified delay distribution.

Original paper: 
https://www.researchgate.net/publication/249650753_Blind_deconvolution_by_means_of_the_Richardson-Lucy_algorithm

Current approach: translating a python version of RL blind deconvolution to R to see if I can improve upon my performance: 
https://github.com/scikit-image/scikit-image/pull/3524/commits/e84b1537e35667b7da13c56a72e2c53392c90f7d

Observations:
- this always outputs essentially the same thing for the same inputs - so it is producing *consistent* (but incorrect) results


```{r}
rm(list = ls())
library(ggplot2)
library(dplyr)
library(tidyr)
library(readr)
library(cowplot)
library(EpiEstim)
library(fs)
library(magic)
library(corpcor)
theme_set(theme_bw())

#the experiments in this folder with no max_RL_param specified in title have max_RL_param = 5
delay_distr_shape_mult_param <- 1.1 #going to assume a delay distribution with shape that is delay_distr_shape_mult*true_shape, scale is delay_distr_scale_mult*true_scale
delay_distr_scale_mult_param <- 1

#Note: mean of gamma distribution is shape*scale; variance is shape*scale^2; then we are adding a uniformly distributed delay between zero and 4 days on top of the gamma distribution.

intervention_length_name <- sprintf("_shape_%s_scale_%s_4", toString(delay_distr_shape_mult_param), toString(delay_distr_scale_mult_param))
intervention_file_name <- sprintf("intervention%s", intervention_length_name)
if (!dir.exists(intervention_file_name)) {
  dir.create(intervention_file_name)
}
```


##Richardson-Lucy method for deconvolving delay distribution and observations of cases

```{r}
if (file.exists(sprintf("%s/params.Rds", intervention_file_name))) {
  parlist <- readRDS(sprintf("%s/params.Rds", intervention_file_name))
} else {
  ## Set parameters to simulate SEIR epidemic
  parlist <- {
    list(
      N = 2e6, #total population size
      E_init = 0,
      I_init = 10,
      t_E = 4, # mean time in E (latent period)
      t_I = 4, # mean time in I (duration of infectiousness)
      n_t = 300, # total timesteps
      pre_intervention_R0 = 2, # Initial R0 before interventions
      intervention_R0 = 0.8, # Final R0 after interventions
      partially_lifeted_R0 = 0.9,
      intervention_time_1 = 62, # Timepoint at which intervention starts (at which underlying transmission rate begins to fall)
      intervention_time_2 = 62+200,
      days_intervention_to_min = c(7), # Days from intervention start until transmission rate hits min_R0
      days_to_Rt_rise = 1,
      model_types = c('seir'), # Can also choose sir
      methods = c('stochastic'), # could also choose ode, but would have to modify it to have integer case counts
      obs_1_offset = 10, #the first observation is at 1+length(delay dist)+obs_1_offset
      obs_2_offset = 75, #the last observation is at 1+length(delay dist)+obs_1_offset+obs_2_offset
      max_RL_it = 10,
      delay_distr_shape_mult = delay_distr_shape_mult_param,
      delay_distr_scale_mult = delay_distr_scale_mult_param,
      num_blind_it = 5,
      num_delay_it = 5,
      num_u_it = 10
    )
  }  
  ## Derive the mean and variance of the serial interval from the input parameters
  parlist$true_mean_SI = (parlist$t_E+parlist$t_I)
  parlist$true_var_SI = 2*parlist$true_mean_SI^2
  saveRDS(parlist, file = sprintf("%s/params.Rds", intervention_file_name))
}


```


## Write function to estimate $R_t$ - Cori method
```{r}
## Output cori estimate with mean, CI and times given an input df, and the name of the incidence column
# Note that here, we're using the option that samples over serial intervals and std of serial intervals
get_cori <- function(df.in, 
                     icol_name, 
                     out_name = 'Cori',
                     window = 1, 
                     SI_mean=parlist$true_mean_SI, 
                     SI_var= parlist$true_var_SI, #changed this from 2*(parlist$true_mean_SI/2)^2,
                     wend = FALSE){
  idat <- df.in %>%
    filter(get(icol_name) > 0 & !is.na(get(icol_name))) %>%
    complete(time = 2:max(time))%>%
    mutate_all(.funs = function(xx){ifelse(is.na(xx), 0, xx)})
  
  ts <- idat$time
  ts <- ts[ts > 1 & ts < (max(ts)-window+1)]
  te<- ts+(window-1)
  
  estimate_R(
    incid = pull(idat, eval(icol_name)),
    method = "parametric",
    config = make_config(
      list(
        mean_si = SI_mean,
        #min_mean_si = SI_mean -1,
        #max_mean_si = SI_mean + 1,
        #std_mean_si = 1.5,
        #std_std_si = 1.5,
        std_si = sqrt(SI_var),
        #min_std_si = sqrt(SI_var)*.8,
        #max_std_si = sqrt(SI_var)*1.2,
        #n1 = 50,
        #n2 = 100, 
        t_start=ts,
        t_end=te
        )
      )
    ) -> outs
  
  outs$R %>%
    mutate(time = if(wend == TRUE) t_end else ceiling((t_end+t_start)/2) ) %>%
    select(time, `Mean(R)`, `Quantile.0.025(R)`, `Quantile.0.975(R)`) %>%
    setNames(c('time', paste0(out_name, '.mean'), paste0(out_name, '.025'), paste0(out_name, '.975')))
}
```

## Simulate SEIR data

```{r}
## Simulate SEIR data using a stochastic (ode) model. Putting the R0 and simplots folders in the intervention file.
source('funs_simulation-sweep.R')
sim_sweep(parlist)
testplots(parlist)

file_move("R0-2.0", intervention_file_name)
file_move("simplots", intervention_file_name) 
```

## Extract SEIR simulation and plot.

- Incidence here is observed at time of infection (S -> E).
- SEIR simulation is stochastic.

```{r}
## Write a function to extract the simulation results as a data frame
stoch_df <- function(){
  readRDS(sprintf('%s/R0-%.1f/seir_%s_dec%.0f-%.0f_sim.rds',
                  intervention_file_name,
                  parlist$pre_intervention_R0, 
                  parlist$methods,
                  parlist$intervention_time_1, 
                  parlist$days_intervention_to_min))$sim_df 
}

stoch_df() %>%
ggplot() +
  geom_line(aes(x = time, y = incidence))+
  geom_vline(aes(xintercept = parlist$intervention_time_1), lty = 2)+ ## Dashed line where Rt starts to decrease
    geom_vline(aes(xintercept = parlist$intervention_time_2), lty = 2)+ ## Dashed line where Rt starts to decrease
  ggtitle('Daily infections, SEIR simulation') -> inc

stoch_df() %>% 
  ggplot()+
  geom_line(aes(x = time, y = true_r0)) +
  geom_hline(aes(yintercept = 1), lty = 2)+
  ylab('R0')+
  ggtitle('Input Rt values') -> R0

plot_grid(R0, inc, rel_heights = c(1,2), align = 'hv', nrow = 2)

stoch_df
```
## Forward step: Impute times of observation from SEIR times of infection 

```{r}
source('funs_impute_obs_times.R')

## Set the delay distribution to observation
obs_delay_dist <- function(nn){
  r_inc_dist <- function(n){rgamma(n, shape = 5.8, scale = 0.95)} # Incubation period (infection -> symptoms)
  r_sym_to_obs_dist <- function(n){runif(n, 0, 4)} # Additional delay from symptoms -> observation
  r_inc_dist(nn) + r_sym_to_obs_dist(nn)
#  rep(20, nn)
}

misspec_obs_delay_dist <- function(nn){
  r_inc_dist <- function(n){rgamma(n, shape = 5.8*parlist$delay_distr_shape_mult, scale = 0.95*parlist$delay_distr_scale_mult)} # Incubation period (infection -> symptoms)
  r_sym_to_obs_dist <- function(n){runif(n, 0, 4)} # Additional delay from symptoms -> observation
  r_inc_dist(nn) + r_sym_to_obs_dist(nn)
#  rep(20, nn)
}

## Append number of new daily infections to simulation dataframe
sim_df <- stoch_df() %>%
  filter(time < max(time)) %>%
  mutate(
    new_infected = ifelse(is.na(dS), 0, dS))
```

## Define observation step
```{r, echo=FALSE}
#get_tObs_from_tInf(sim_df$new_infected, sim_df$time, obs_delay_dist, return_times = T)
sim_df %>%
  merge(
    get_tObs_from_tInf(sim_df$new_infected, sim_df$time, obs_delay_dist, return_times = T),
    by = 'time', all = TRUE) %>% 
  rename(new_observed = n) %>%
  as.tbl() -> sim_df

```

### Deconvolving using Richardson-Lucy method 

Plotting the delay distribution. *Assuming a misspecified delay distribution here.*
  
```{r, echo=FALSE}
#THE CORRECT DELAY DISTRIBUTION
obs_delay_dist(10000000)  %>%
#    ceiling() %>%
  tabulate()*1/10000000 -> true_delay_distr_vec #could also set nbins = const, which would set the length of the delay distribution to always be const, at the expense of potentially having 0's

#ASSUMING A MISSPECIFIED DELAY DISTRIBUTION
misspec_obs_delay_dist(10000000)  %>%
#    ceiling() %>%
  tabulate()*1/10000000 -> delay_distr_vec #could also set nbins = const, which would set the length of the delay distribution to always be const, at the expense of potentially having 0's

len_diff <- length(sim_df$new_infected)-length(delay_distr_vec)

#length(delay_distr_vec)

#plot(1:length(delay_distr_vec), delay_distr_vec)

```

```{r}

#delay_distr_vec <- c(delay_distr_vec, replicate(len_diff,0))
#length(delay_distr_vec)

delay_distr_vec
```

```{r}

new_inf_no_na <- sim_df$new_infected #number of cases at the S -> E transition. This is what RL will be trying to reproduce.
#new_obs_no_na <- ifelse(is.na(sim_df$new_observed), 0, sim_df$new_observed)
#obs_cases <- get_tObs_from_tInf(new_inf_no_na, 
#                              times=1:length(new_inf_no_na), 
#                               r_delay_dist=obs_delay_dist,
#                               return_times = FALSE)
new_obs_no_na <- sim_df$new_observed #this is the "data" vector we will apply RL to. 


```

## Edge improvement strategy: Careful zero-padding

*Reference*: Bertero & Boccacci	https://doi.org/10.1051/0004-6361:20052717
  
length(init_guess) should be L' = length(obs_dat) + 2*length(ker_vec) - 1
A goes from inferred to measured: dim L' x L
  
zero padding: init_guess_pad <- c(rep(0, L-(L'/2)), guess, rep(0, L-(L'/2)))
zero padding: obs_dat_pad <- c(rep(0, L/2), obs_dat, c(0, L/2))
zero padding: A' <- cbind(matrix(0, dim(A)[1],L-L'/2), A, matrix(0, dim(A)[1], L-L'/2))
              #  A' <- rbind(matrix(0, L' + L/2), A', matrix(0, L'+ L/2))
              dim(A') = 2L x 2L
Note: this all implicitly enforces that obs2_offset > len(kernel)-1

Then, define: alpha_i (dim L' by 1) = sum_{j = 1}^{L} A_{ij}, which are the equivalent of the "q"'s in the current implementation. 

Take $$window_i = 1/\alpha_i \text{ for } \alpha_i > \text{threshold; otherwise,} 0$$

RL iteration: 
$$ u^{k+1}_i = w_i u^k_i \left(p^k \otimes \frac{dat}{u^k \otimes p^k}\right)_i $$

To do blind deconvolution taking into account the image boundaries appropriately: 
Without boundaries - 
*Reference*: Fish et al, 1995 https://www.osapublishing.org/josaa/abstract.cfm?uri=josaa-12-1-58
[blurred image], [true image], [blurring kernel]
start with - 
[blurred image], [true image]_0, [blurring kernel]_0
do - 
[blurring kernel]_1 = [blurring kernel]_0 * ([true image]_0 * [blurring image]/([true_0] ** [blurring kernel]_1)
[true image]_1 = [true image]_1 * ([blurring kernel]_1 * [blurring image]/([true_0] ** [blurring kernel]_1))

Non-blind with boundaries - 
start with - 
[blurred image, boundary], [true image, no boundary]_0, [blurring kernel, no boundary]_0 
do 
[expanded blurred image, no boundary], [expanded true image, no boundary]_0, [blurring kernel, no boundary]_0 
sizes: 2L, 2L, 2Lx2L (when converting blurring kernel to matrix via circulant)
recovered images: 
take center([expanded true image, no boundary]_final) so that boundaries are same as with original [blurred image]

Combining the two, blind with boundaries: 
start with 
[blurred image, boundary], [true image, no boundary]_0, [blurring kernel]_0
first step: hold two variables constant and do the appropriate RL - 
[blurred image, boundary] - L,  [blurring kernel]_0 - p, [true image, no boundary]* - L x L' (= L + 2p-2)
[expanded blurred image, boundary] - L', [expanded blurring kernel]_0, [expanded true image, no boundary]*
do - 
non-blind, with boundaries, treating [true image, no boundary]_0 as a fixed, known quantity (circulant matrix)
then 
do 
non-blind, with boundaries, treating [blurring kernel]_1 as a fixed, known quantity (circulant matrix)
then
iterate
until
stopping

Beginning input: 
- observed [blurred] incidence curve [image]
- guess for delay distribution [blurring kernel]

call an iteration of RL - 
To set up the matrices:
- define which matrices are data or fixed and which is being solved for 
- create the "kernel" matrix 
- fix boundaries of everything by zero padding
Iterate RL

Switch the matrices which are being treated as constant and which are being iterated over

Repeat how ever many times


```{r}
get_chi_sq <- function (vec1, vec2) {
  n0 <- length(vec1)
  vec1_mod <- ifelse(vec1 < 0.00000001, 0.00001, vec1)
  (1/n0)*sum(((vec1 - vec2)^2/vec1_mod)) 
}

do_RL <- function (init_guess, obs_dat, ker_vec, obs_1, obs_2, max_it){ #no longer have to feed this an initial guess?
  lend <- length(obs_dat) #L
  print("lend")
  print(lend)
  lenk <- length(ker_vec)
  print("lenk")
  print(lenk)
  if (lenk %% 2 == 1){
    ker_vec <- c(ker_vec, 0)
    lenk <- length(ker_vec)
  }
  print(lenk)
  #init_guess <- c(obs_dat, rep(obs_dat[lend], lenk))
  leninit <- length(init_guess) #L'
  print("leninit")
  print(leninit)
  ker_mat_big<- circulant(c(ker_vec, rep(0, leninit-lenk)))
  ker_mat <- ker_mat_big[1:lend, ]
  print("here 1")
  
  obs_dat_pad <- c(rep(0, lend/2), obs_dat, rep(0, lend/2))
  init_guess_pad <- c(rep(0, lend-(leninit/2)), init_guess, rep(0, lend-(leninit)/2))
  ker_mat_pad_0 <- cbind(matrix(0, dim(ker_mat)[1], lend-leninit/2), ker_mat, matrix(0, dim(ker_mat)[1], lend-leninit/2))
  ker_mat_pad <- rbind(matrix(0, lend/2, dim(ker_mat_pad_0)[2]), ker_mat_pad_0, matrix(0, lend/2, dim(ker_mat_pad_0)[2]))
  print("here 2")
  
  alpha_pad <- colSums(ker_mat_pad)
  window_pad <- ifelse(alpha_pad < 0.00001, 0, 1/alpha_pad)

  curr_guess <- init_guess_pad
  
  chi_sq <- get_chi_sq(obs_dat_pad, curr_guess)
  ind <- 1
  
  print("here 3")
  
  while (chi_sq > 1 & ind < max_it) {
    #c_obs <-p_ij_obs_rescaled %*% u_obs_rescaled
    #new_kernel_obs <- d_obs/c_obs
    #new_u_obs_rescaled <- u_obs_rescaled * t(t(new_kernel_obs) %*% p_ij_obs_rescaled)
        #u_obs_rescaled <- new_u_obs_rescaled
    
    obs_guess <- ker_mat_pad %*% curr_guess
    new_ker <- ifelse(!is.na(obs_dat_pad/obs_guess), obs_dat_pad/obs_guess, 0) 
    new_guess <- window_pad * curr_guess * t(t(new_ker) %*% ker_mat_pad) 
    curr_guess <- new_guess
    chi_sq <- get_chi_sq(obs_dat_pad, curr_guess) 
    ind <- ind+1
  }
  print("lend")
  print(lend)
  print("leninit")
  print(leninit)
  print(lend-leninit/2)
  curr_guess[(lend-(leninit)/2+1):(length(curr_guess)-lend-(leninit)/2)]
}
```


##Blind Deconvolution Strategy 

Doing a set of RL iterations. 
Change: Be more careful about rescaling and normalization. 
Logic: Iterate... 
1. Solve for PSF, holding u fixed 
2. Solve for u, holding PSF fixed 
i.e., in step 1, feed the RL algorithm $u$ such that $\sum_i u_i = 1$ and carry out normal RL on the PSF, then, in step 2, feed the RL algorithm $u$
Should always have it be the case that both variables are normalized when doing RL deconvolution. 


Current logic: 
Start with observed data, dat.
Let u = observed, p = delay. Take: 
0_M, 0, 0, ..., 0_0, u_1, u_2, u_3, ..., u_N, 0_1, 0, 0, ..., 0_M
p_M p_{M-1} ... p_0 , 0_1, 0, ..., 0_N, 0_1, ..., 0_{M-1}
expanded dat: 
0_M, ..., 0_1, d_1, ..., d_N, 0_1, ..., 0_M - i.e., expand dat by length(p)-1 [or, in the other case, length(u)-1] on either side [this is how to deal with the edge issues] - these are all the pixels that contribute to the pixels in the image we see
guess for u: 
d_1, ..., d_N, d_N, ..., d_N
guess for p: misspecified assumed p ->
p_o, ..., p_m, 0, ..., 0 
the claim is that: 
every pixel coming into or out of d is contained within this interval, 
meaning: sum(u) = sum(dat) (and we are already assuming sum(p) = 1)
i.e. take everything to be divided by sum(dat) - i.e., by the total number of infections observed, and then everything should be normalized throughout
Do the final right-truncation adjustment on u later.

In the process of: 
Changing the zero padding to be within do_RL such that it correctly matches the Bertero-Boccacci method.

```{r}
p_unnorm <- c(0.00000001, ifelse(delay_distr_vec==0, 0.00000001, delay_distr_vec)) #the first element is the probability of delay = 0
p <- p_unnorm/sum(p_unnorm)
#plot(p)

d <- new_obs_no_na
#plot(d)

#obs_1_offset is how long you wait after the first serial interval to start measuring
obs_1 <- length(p)+1+parlist$obs_1_offset
obs_1
obs_2 <- obs_1 + parlist$obs_2_offset
obs_2

dat <- d[obs_1:obs_2]
length(rep(0.00000001, length(p)-1)) #one less than the length of p
length(p)

#HERE, dat is not actually expanded at all! Because we're going to do the expanding within the RL loop
#dat_expanded_unnorm <- c(rep(0.00000001, length(p)-1), d[obs_1:obs_2], rep(0.00000001, length(p)-1))
dat_expanded_unnorm <- d[obs_1:obs_2] #do I want to pad with zeros on the left to make the timing match with u, or take that into account in the timing indexing later?
dat_expanded <- c(dat_expanded_unnorm/sum(dat_expanded_unnorm))

#plot(dat)
length(dat)
#plot(dat_expanded)
length(dat_expanded)


```

```{r}
#plot(dat)
#dat[length(dat)]
u_guess_unnorm <- c(dat, rep(dat[length(dat)], length(p)-1)) #this is the guess for the larger "image" contributing to the "image" (dat) with the edge; dat is smaller because it is cut-off version of blurred u
#plot(u_guess)
u_guess <- u_guess_unnorm/sum(u_guess_unnorm) #expands dat on either side, but with respect to p, not with respect to the zero padding we do later in the edge handling
length(u_guess)
length(dat)

p_guess_unnorm <- c(p, rep(0.00000001, length(dat)-1)) #we're adding 0's to the end of p_guess; not sure this is the right thing to do here?
p_guess <- p_guess_unnorm/sum(p_guess_unnorm)
#plot(p_guess)
sum(p_guess)
length(p_guess)
p_guess

circ_test <- circulant(p_guess) #it's possible that this should be t(circulant(p_guess))
#circ_test[92,]
#plot(p_guess)
#print(circ_test)

length(p)
length(dat_expanded)
length(p_guess)
p_guess #p with a bunch of 0's added on in order to make it the same length as dat - i.e., L
```

```{r}
this_u <- u_guess #already normalized above
#sum(this_u)
this_delay <- p_guess #already normalized
#sum(this_delay)

plot(this_u)
```

For testing, just do one iteration of each.

```{r}
num_delay_it <- 1 #parlist$num_delay_it
num_u_it <- 1 #parlist$num_u_it
num_blind_it <- 1 #parlist$num_blind_it
```

The Fish 1995 publication claims one compute the new PSF using the original data, but the example python code referenced in the beginning claims it works better to use the most-recent deconvolved u instead, so that's what I'm trying here.

```{r}
k = 1
#plot(this_u)
#points(dat, col = "blue")
#points(u_guess, col = "red")

length(this_u) 
length(dat)
this_u 

length(dat) 
#plot(dat)

#plot(this_u)
#points(dat, col = "blue") 
#points(u_guess, col = "red")

#plot(this_delay)
length(this_delay) 

this_delay

for (k in 1:num_blind_it) {
  new_delay <- do_RL(this_delay, this_u, this_u, obs_1, obs_2, num_delay_it)
  this_delay <- new_delay 
  print("new_delay")
  print(this_delay)
  new_u <- do_RL(this_u, dat, this_delay, obs_1, obs_2, num_u_it)
  this_u <- new_u 
}

plot(this_delay)

plot(this_u)
points(dat, col = "blue")
points(u_guess, col = "red")


plot(this_delay[1:40])
points(true_delay_distr_vec, col = "blue")
points(delay_distr_vec, col = "red")
```

```{r}
u_obs_new <- this_u*sum(u_guess_unnorm)
inferred_df <- data.frame(c(obs_1 - length(p) + 1):obs_2, u_obs_new)
names(inferred_df) <- c('time', "new_inferred")
#print(inferred_df)

#TRY REDEFINING THE TIME SHIFTS HERE##############################
inferred_df$time <- inferred_df$time - 1
sim_df$time <- sim_df$time + 1

sim_df %>%
  merge(inferred_df, by = 'time', all = 'TRUE') %>%
  as.tbl()  %>%
  pivot_longer(-time, names_to = "data_type", values_to = "count") %>% 
  filter(data_type == "new_inferred"| data_type == "new_infected" | data_type == "new_observed") %>%
  ggplot() +
  geom_line(aes(x = time, y = count, color = data_type, linetype = data_type)) + 
  geom_vline(aes(xintercept = parlist$intervention_time_1), linetype = "dotted") +
  scale_linetype_manual(values=c("solid", "solid", "dotted")) + 
  scale_color_manual(values = c("blue", "red", "green")) +
  labs(color = "Infections", linetype = "Infections") + 
  ylab("count")+
  xlab("time (days)") + 
  ggtitle('Number of inferred cases from Richardson-Lucy') -> inferred_plot

inferred_plot

ggsave(sprintf("%s/all-case-curves.png", intervention_file_name))

```

The red lines mark the beginning and end of where we have data.

```{r}
# 
# 
# diff <-  u_obs_new - u_true[(obs_1 - length(p) + 1):obs_2]
# ggplot()+
#   geom_line(aes(x=(c(obs_1 - length(p) + 1):obs_2), diff, colour = 'red')) +
#   ylab("difference in number of infections")+
#   xlab("time (days)") + 
# #  geom_vline(aes(xintercept = obs_1, colour = 'red'), linetype = "dashed") + 
# #  geom_vline(aes(xintercept = obs_2, colour = 'red'), linetype = "dashed") + 
#   geom_hline(aes(yintercept = 0, colour = 'black'), linetype = "dashed")+
#   geom_vline(aes(xintercept = parlist$intervention_time_1, color = "blue"), linetype = "dashed") +
#   scale_color_discrete(name = "Legend", labels = c("Difference = 0", "Time of intervention", "Infections, inferred minus true")) +
#   ggtitle('Difference, inferred infections minus true infections, Richardson-Lucy')
# 
# ggsave(sprintf("%s/difference-cases-RL.png", intervention_file_name))
```

```{r}
u_true <- new_inf_no_na 
#diff <-  u_obs_new - u_true[(obs_1 - length(p) + 1):obs_2]

diff <-  u_obs_new - u_true[(obs_1 - length(p)):(obs_2-1)]

sim_df2 <- sim_df
diff_df <- data.frame(c(obs_1 - length(p) + 1):obs_2, diff)
names(diff_df) <- c('time', "diff")

sim_df2 %>%
  merge(diff_df, by = 'time', all = 'TRUE') %>%
  as.tbl()  %>%
  pivot_longer(-time, names_to = "data_type", values_to = "count") %>%
  filter(data_type == "diff" & !is.na(count)) %>%
  ggplot() +
  geom_line(aes(x = time, y = count, color = data_type, linetype = data_type)) + 
  geom_hline(aes(yintercept = 0, color = 'zero', linetype = 'zero'))+
  geom_vline(aes(xintercept = parlist$intervention_time_1, color = 'intervention', linetype = 'intervention')) +
  scale_linetype_manual(values=c("solid", "dotted", "dashed")) + 
  scale_color_manual(values = c("blue", "black", "black")) +
 # scale_color_discrete(name = "Legend", labels = c("Difference = 0", "Time of intervention", "Infections, inferred minus true")) +
  labs(color = "Legend", linetype = "Legend") +
  ylab("count (difference)")+
  xlab("time (days)") + 
  ggtitle('Difference in number of inferred cases from Richardson-Lucy') -> diff_plot

diff_plot

ggsave(sprintf("%s/difference-cases-RL.png", intervention_file_name))

```
```{r}


cori_df <- get_cori(df.in = sim_df, icol_name = "incidence")
cori_inferred <- get_cori(df.in = inferred_df, icol_name = "new_inferred")


# cori_df %>%
#   filter(!is.na(Cori.mean)) %>%
#   ggplot() + 
#   geom_line(aes(x = time, y = Cori.mean)) + 
#   geom_ribbon(aes(x = time, ymin=Cori.025, ymax = Cori.975), alpha=0.3)
# 
# cori_inferred %>%
#   filter(!is.na(Cori.mean) & Cori.mean < 5) %>%
#   ggplot() + 
#   geom_line(aes(x = time, y = Cori.mean)) + 
#   geom_ribbon(aes(x = time, ymin=Cori.025, ymax = Cori.975), alpha=0.3)

all_cori_df <- merge(cori_df, cori_inferred, by = "time")
r0_df <- stoch_df()
merge(all_cori_df, r0_df, by = "time") -> new_all_df

new_all_df %>%
  filter(!is.na(Cori.mean.x) & !is.na(Cori.mean.y) & Cori.mean.y < 5 & Cori.975.y < 8) %>%
  ggplot() + 
  geom_hline(aes(yintercept = 1, linetype = "one", color = "one")) + 
  geom_line(aes(x = time, y = true_r0, color = "true_Rt", linetype = "true_Rt"), size = 1) +
  geom_line(aes(x = time, y = Cori.mean.x, color = "true_infections", linetype = "true_infections")) + 
  geom_line(aes(x = time, y = Cori.mean.y, color = "inferred_infections", linetype = "inferred_infections")) +
  geom_ribbon(aes(x = time, ymin=Cori.025.x, ymax = Cori.975.x, fill = "true_infections"), alpha=0.3, show.legend = FALSE) + 
  geom_ribbon(aes(x = time, ymin=Cori.025.y, ymax = Cori.975.y, fill = "inferred_infections"), alpha=0.3, show.legend = FALSE) +
  geom_vline(aes(xintercept = parlist$intervention_time_1, color = 'intervention', linetype = 'intervention')) + 
  scale_linetype_manual(values=c("solid", "dotted", "dashed", "solid", "solid")) + 
  scale_color_manual(values = c("red", "black", "black", "blue", "black")) +
  scale_fill_manual(values = c("red", "blue")) +
 # scale_color_discrete(name = "Legend", labels = c("Difference = 0", "Time of intervention", "Infections, inferred minus true")) +
  labs(color = "Legend", linetype = "Legend", fill = "Legend") +
  xlab("time (days)") + 
  ylab("Rt") +
  ggtitle('Calculations of Rt') -> new_all_plot

new_all_plot

ggsave(sprintf("%s/rt_plot.png", intervention_file_name))
```


```{r}

plot_grid(inferred_plot, diff_plot, new_all_plot, labels = "AUTO", ncol = 1, align = 'v') -> all_plot

all_plot

ggsave(sprintf("%s/all_plot.png", intervention_file_name))

```
```{r}
cori_x_no_na <- new_all_df$Cori.mean.x[!is.na(new_all_df$Cori.mean.x)]
fft_x <- abs(fft(cori_x_no_na))
plot(abs(fft(cori_x_no_na))/fft_x[1])
#print(new_all_df$Cori.mean.x)

cori_y_no_na <- new_all_df$Cori.mean.y[!is.na(new_all_df$Cori.mean.y)]
fft_y <- abs(fft(cori_y_no_na))
points(abs(fft(cori_y_no_na))/fft_y[1], col = "red")
#print(new_all_df$Cori.mean.y)
```




```{r} 
do_RL_no_edge_handling <- function (init_guess, obs_dat, ker_vec, obs_1, obs_2, max_it){
  ker_mat_unnorm <- get_ker_ij(ker_vec, obs_1, obs_2) 
  q_j <- colSums(ker_mat_unnorm) #
#  print(q_j)
  dim_ker_mat <- dim(ker_mat_unnorm) #always 76 x 93 - length(observation time) x [length(obs) and length_init]
  print(dim_ker_mat) 
  ker_mat <-  ker_mat_unnorm #/ matrix(q_j,nrow=dim_ker_mat[1],ncol=dim_ker_mat[2],byrow=TRUE)
  #u_obs_rescaled <- u_obs_guess_rescaled
  
  len_ker <- length(ker_vec)
  curr_guess <- init_guess #* q_j
  
  #I'm doing the zero padding within the do_RL function, so the data/kernel I feed this should not already be zero-padded?
  #obs_dat_re <- c(rep(0.000001, len_ker-1), obs_dat, rep(0.000001, len_ker-1)) #* q_j[(length(q_j) - length(obs_dat) + 1):length(q_j)]
  obs_dat_re <- obs_dat
  print(length(obs_dat_re)) #always the length of obs_dat
  
  chi_sq <- get_chi_sq(obs_dat_re, curr_guess[len_ker:length(curr_guess)])
  ind <- 1
  
  while (chi_sq > 1 & ind < max_it) {
    #c_obs <-p_ij_obs_rescaled %*% u_obs_rescaled
    
    obs_guess <- ker_mat %*% curr_guess #76x93 * 93x1 = 76x1
    #print(c_obs)
    
    #new_kernel_obs <- d_obs/c_obs
    new_ker <- obs_dat_re/obs_guess #93x1/76x1 ?!?!?!?!?!?!?!?!
    #print(t(new_kernel_obs)%*%p_ij_obs_rescaled)
    
    #print(u_obs_rescaled)
    #new_u_obs_rescaled <- u_obs_rescaled * t(t(new_kernel_obs) %*% p_ij_obs_rescaled)
    #93 x t(t(new)%*% ker_mat) = [vec] * t(t(93? 76?) %*% 76 x 93) = [93x1] * t(t(1 x 76 % 76 x 93)) = [93x1] * [93x1]  
    new_guess <- curr_guess * t(t(new_ker) %*% ker_mat) # 93 x 1 because first multiplication is ELEMENT-WISE
    #print(new_u)
    
    #u_obs_rescaled <- new_u_obs_rescaled
    curr_guess <- new_guess
    
    chi_sq <- get_chi_sq(obs_dat_re, curr_guess[len_ker:length(curr_guess)]) #this isn't really doing anything right now
    ind <- ind+1
  }
  curr_guess
}

#ker_ij knows how to cut off the matrix based on the observation times
get_ker_ij_no_edge_handling <- function (this_ker, obs_time_1, obs_time_2){
  ker_mat <- circulant(c(this_ker, rep(0, obs_time_2)))
  ker_ij_obs <- ker_mat[(obs_time_2-length(this_ker)+1):obs_time_2, obs_time_1:obs_time_2]
  #p_ij_obs <- pmat[obs_1:obs_2, 1:length(u_obs_guess)]
  ker_ij_obs <- t(ker_ij_obs)
}

```