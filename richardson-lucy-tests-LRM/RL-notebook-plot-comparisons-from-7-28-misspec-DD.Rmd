---
title: "Richardson-Lucy Deconvolution Methods for Back-calculating Incidence data: Comparing Methods"
output: html_notebook
---

RL deconvolution analysis by Lauren McGough, Simulations adapted from Katie Gostic

Last updated: 7-31-2020

## Introduction

One of the major challenges in computing $R_t$ from observed data is that $R_t$ calculations depend on incidence at the time of $\textit{infection}$, whereas available data is always delayed, whether measured at the time of symptom onset, hospitalization, death, or other times during the course of an individual's infections. 
Moreover, this delay distribution is probabilistic, and typically not known, although it may be estimated based on assumptions and empirical measurements of the course of the disease and the measurement process itself. 

The mathematical operation needed in order to recover incidence at the time of infection from a set of data that has been subject to delay is $\textit{deconvolution}$, although many current methods in use effectively amount to $\textit{convolution}$, thus producing an incorrect distribution of infection curves. There are many mathematical methods known to carry out deconvolutions, all subject to different drawbacks depending on noise in the system and other unknown variables. 

One method which has been previously shown to be successful for recovering incidence curves from observed data is the Richardson-Lucy method (Goldstein et al), an iterative method which begins with a guess for the input curve and interates to subsequently improve the deconvolved data. The original RL method depends on the specification of a known delay distribution, although it may be possible to generalize to cases where the delay distribution is unknown through joint estimation of the delay distribution and the incidence curve. 

Here, we apply the RL method to synthetic data generated from stochastic SEIR models in order to assess its performance on synthetic data where the input curve is known exactly, beginning with the traditional RL method and then assessing its performance in the case where the delay distribution is unknown or misspecified.

Experimenting with incorrect delay distributions, we find: 
* Deconvolution does well up to shape factor ~1.6 times true shape factor
* Much less robust to scale factor errors
* More robust when multiplier is <1 for both shape and scale

Note: For $k$ shape parameter, $\theta$ scale parameter, mean of delay distribution is $0 + k\theta$, variance is $0 + k\theta^2$.
(The constants come from the inclusion of a uniform distribution on $[0,4]$ in addition to the gamma distribution.)

For multiplying mean by a, keeping variance fixed:
s p k t = a k t
s p^2 k t^2 = k t^2 

s p = a
a p = 1
p = 1/a - scale multiplier for t - scale param
s = a^2/1 - scale multiplier for k - shape param

For multiplying variance by a^2, keeping mean fixed:
s p k t = k t
s p^2 k t^2 = a^2 k t^2
s p = 1
p = a^2 = multiplier for scale param
s = 1/a^2 =  multiplier for shape  param

Multiplying mean by 1.1, keeping variance fixed:
for scale - 1/1.1
for shape - 1.1^2

Multiplying mean by 1.6, keeping variance fixed:
for scale - 1/1.6
for shape: 1.6^2

Multiplying variance by 1.1^2, keeping mean fixed:
for scale - 1/1.1^2
for shape - 1.1^2

Multiplying mean by a, variance by b^2:
s p k t = a k t
s p^2 k t^2 = b^2 k t^2
s p = a
s p^2 = b^2 
p = b^2/a
s = a^2/b^2

Multiplying mean by 1.1, variance by 1.6^2:
shape multiplier = 1.1^2/1.6^2
scale multiplier = 1.6^2/1.1

30 days mean, 20 day variance:
k *t = 30 
k * t^2 = 20
30*t = 20 -> t = 2/3
k * 2/3 = 30 -> k = 90/2 = 45

30 day mean, 10 day variance:
k * t = 30 
k * t * t = 10
t = 10/30 = 1/3
k * 10 / 30 = 30 -> k = 30*30 / 10 = 90 

30 day mean, 5 day variance:
k * t = 30 
k * t * t = 5
t = 5/30 = 1/6
k * 1 / 6 = 30 -> k = 30*6 = 180 

## Major lesson

When mean is misspecified but variance is perfectly specified: both RL and shifted-observed curves are essentially translated according to the new mean, and the RL curve always gets the shape of the infections better than the shifted observed curve.(Until mean is very misspecified, i.e. by a factor of 2)

When variance is misspecified: depends on the extent of the misspecification. If misspecification $\lesssim 10\%$, it's better to do the deconvolution using RL, but for higher misspecifications, RL performs worse on inferring infections than just shifting the observed curve.


```{r}
rm(list = ls())
library(ggplot2)
library(dplyr)
library(tidyr)
library(readr)
library(cowplot)
library(EpiEstim)
library(fs)
library(magic)
library(corpcor)
theme_set(theme_bw())
```


```{r}
#intervention_length_name <- sprintf("_shape_%s_scale_%s_2", toString(delay_distr_shape_mult_param), toString(delay_distr_scale_mult_param))
intervention_length_name <- "_mean_30_var_20_shape_mult_1.1sq_over_1.6sq_scale_mult_1.6sq_over_1.1"
intervention_file_name <- sprintf("intervention%s", intervention_length_name)
if (!dir.exists(intervention_file_name)) {
  dir.create(intervention_file_name)
}
```

```{r}
if (file.exists(sprintf("%s/params.Rds", intervention_file_name))) {
  parlist <- readRDS(sprintf("%s/params.Rds", intervention_file_name))
} else {
  ## Set parameters to simulate SEIR epidemic
  parlist <- {
    list(
      N = 2e6, #total population size
      E_init = 0,
      I_init = 10,
      t_E = 4, # mean time in E (latent period)
      t_I = 4, # mean time in I (duration of infectiousness)
      n_t =600, # total timesteps
      pre_intervention_R0 = 2, # Initial R0 before interventions
      intervention_R0 = 0.8, # Final R0 after interventions
      partially_lifeted_R0 = 0.9,
      intervention_time_1 = 150, # Timepoint at which intervention starts (at which underlying transmission rate begins to fall)
      intervention_time_2 = 150+200,
      days_intervention_to_min = c(7), # Days from intervention start until transmission rate hits min_R0
      days_to_Rt_rise = 1,
      model_types = c('seir'), # Can also choose sir
      methods = c('stochastic'), # could also choose ode, but would have to modify it to have integer case counts
      obs_1_offset = 10, #the first observation is at 1+length(delay dist)+obs_1_offset
      obs_2_offset = 150, #the last observation is at 1+length(delay dist)+obs_1_offset+obs_2_offset
      max_RL_it = 10,
      delay_distr_shape = 45, #mean 30, var 20, 
      delay_distr_scale = 2/3, 
      time_to_obs = 0, #the delay distribution is just a gamma distribution
      delay_distr_shape_mult = 1.1^2/1.6^2,
      delay_distr_scale_mult = 1.6^2/1.1,
      time_to_obs_mult = 1#,
      #num_blind_it = 50,
      #num_delay_it = 10,
      #num_u_it = 20
    )
  }  
  ## Derive the mean and variance of the serial interval from the input parameters
  parlist$true_mean_SI = (parlist$t_E+parlist$t_I)
  parlist$true_var_SI = 2*parlist$true_mean_SI^2
  saveRDS(parlist, file = sprintf("%s/params.Rds", intervention_file_name))
}
```


```{r}
## Simulate SEIR data using a stochastic (ode) model. Putting the R0 and simplots folders in the intervention file.
source('funs_simulation-sweep.R')
sim_sweep(parlist)
testplots(parlist)

file_move("R0-2.0", intervention_file_name)
file_move("simplots", intervention_file_name) 
```


```{r}
## Write a function to extract the simulation results as a data frame
stoch_df <- function(){
  readRDS(sprintf('%s/R0-%.1f/seir_%s_dec%.0f-%.0f_sim.rds',
                  intervention_file_name,
                  parlist$pre_intervention_R0, 
                  parlist$methods,
                  parlist$intervention_time_1, 
                  parlist$days_intervention_to_min))$sim_df 
}

stoch_df() %>%
ggplot() +
  geom_line(aes(x = time, y = incidence))+
  geom_vline(aes(xintercept = parlist$intervention_time_1), lty = 2)+ ## Dashed line where Rt starts to decrease
    geom_vline(aes(xintercept = parlist$intervention_time_2), lty = 2)+ ## Dashed line where Rt starts to decrease
  ggtitle('Daily infections, SEIR simulation') -> inc

stoch_df() %>% 
  ggplot()+
  geom_line(aes(x = time, y = true_r0)) +
  geom_hline(aes(yintercept = 1), lty = 2)+
  ylab('R0')+
  ggtitle('Input Rt values') -> R0

plot_grid(R0, inc, rel_heights = c(1,2), align = 'hv', nrow = 2)

stoch_df
```

## RL with Option for Misspecified Delay Distribution



```{r}
source('funs_impute_obs_times.R')

## Set the delay distribution to observation
obs_delay_dist <- function(nn){
  #r_inc_dist <- function(n){rgamma(n, shape = 5.8, scale = 0.95)} # Incubation period (infection -> symptoms)
  #r_sym_to_obs_dist <- function(n){runif(n, 0, 4)} # Additional delay from symptoms -> observation
  r_inc_dist <- function(n){rgamma(n, shape = parlist$delay_distr_shape, scale = parlist$delay_distr_scale)} # Incubation period (infection -> symptoms)
  r_sym_to_obs_dist <- function(n){runif(n, 0, parlist$time_to_obs)} # Additional delay from symptoms -> observation
  r_inc_dist(nn) + r_sym_to_obs_dist(nn)
#  rep(20, nn)
}

#mean = parlist$time_to_obs*parlist$time_to_obs_mult/2 + shape*scale*scale_multiplier*shape_multiplier

misspec_obs_delay_dist <- function(nn){
  r_inc_dist <- function(n){rgamma(n, shape = parlist$delay_distr_shape*parlist$delay_distr_shape_mult, scale = parlist$delay_distr_scale*parlist$delay_distr_scale_mult)} # Incubation period (infection -> symptoms)
  r_sym_to_obs_dist <- function(n){runif(n, 0, parlist$time_to_obs*parlist$time_to_obs_mult)} # Additional delay from symptoms -> observation
  r_inc_dist(nn) + r_sym_to_obs_dist(nn)
#  rep(20, nn)
}

#parlist$time_to_obs_mult

## Append number of new daily infections to simulation dataframe
sim_df <- stoch_df() %>%
  filter(time < max(time)) %>%
  mutate(
    new_infected = ifelse(is.na(dS), 0, dS))
```

```{r, echo=FALSE}
#get_tObs_from_tInf(sim_df$new_infected, sim_df$time, obs_delay_dist, return_times = T)
sim_df %>%
  merge(
    get_tObs_from_tInf(sim_df$new_infected, sim_df$time, obs_delay_dist, return_times = T),
    by = 'time', all = TRUE) %>% 
  rename(new_observed = n) %>%
  as.tbl() -> sim_df

```

```{r, echo=FALSE}
#ASSUMING A MISSPECIFIED DELAY DISTRIBUTION
misspec_obs_delay_dist(10000000)  %>%
#    ceiling() %>%
  tabulate()*1/10000000 -> delay_distr_vec #could also set nbins = const, which would set the length of the delay distribution to always be const, at the expense of potentially having 0's

len_diff <- length(sim_df$new_infected)-length(delay_distr_vec)

#delay_distr_vec
#length(delay_distr_vec)

#plot(1:length(delay_distr_vec), delay_distr_vec)

#mean(misspec_obs_delay_dist(10000000))

```


```{r}

new_inf_no_na <- ifelse(is.na(sim_df$new_infected), 0, sim_df$new_infected) #number of cases at the S -> E transition. This is what RL will be trying to reproduce.
#new_obs_no_na <- ifelse(is.na(sim_df$new_observed), 0, sim_df$new_observed)
#obs_cases <- get_tObs_from_tInf(new_inf_no_na, 
#                              times=1:length(new_inf_no_na), 
#                               r_delay_dist=obs_delay_dist,
#                               return_times = FALSE)
new_obs_no_na <- ifelse(is.na(sim_df$new_observed), 0, sim_df$new_observed) #this is the "data" vector we will apply RL to. 

#new_obs_no_na
```

```{r}
u_true <- new_inf_no_na 
p_unnorm <- c(0.000001, ifelse(delay_distr_vec==0, 0.000001, delay_distr_vec)) #the first element is the probability of delay = 0
p <- p_unnorm/sum(p_unnorm)
#d <- convolve(u_true, rev(p), type = "open")
#print(d)
d <- new_obs_no_na

#plot(1:length(u_true), u_true)
#points(1:length(d), d, col = 'red')
#points(1:length(new_obs_no_na), new_obs_no_na, col ='blue')

#note: obs_1 must be at least length(p) in this formulation. might find a way to change it so that only have to use most of p
#length(p)
obs_1 <- length(p)+1+parlist$obs_1_offset
obs_2 <- obs_1 + parlist$obs_2_offset
d_obs <- d[obs_1:obs_2]
#d_obs
#length(d_obs)



u_obs_guess <- c(d_obs, rep(max(d_obs[length(d_obs)],1), length(p)-1))
#u_obs_guess

#In appendix to the Lipsitch paper, the conventions are:
#p_ij is indexed by i = number of observations len_d_obs and j = number of "sources" u_obs.
#the normalized \hat{p}_ij is given by p_ij/\sum_i p_ij
#the thing you solve for here is u_guess = q_j*(number of infections)_j
#in order to recover true number of infections, have to divide by u_guess by q_j
#pmat <- circulant(c(p, rep(0, length(p)+length(d_obs)-1)))
pmat <- circulant(c(p, rep(0, obs_2)))
#pmat
#dim(pmat)
p_ij_obs <- pmat[(obs_1-length(p)+1):obs_2, obs_1:obs_2]
#p_ij_obs <- pmat[obs_1:obs_2, 1:length(u_obs_guess)]
p_ij_obs <- t(p_ij_obs)
#p_ij_obs
#p_ij_obs %*% u_obs_guess
#convolve(u_obs_guess, rev(p), type = "open")
pseudo_inv_pij_obs <- pseudoinverse(p_ij_obs)
#pseudo_inv_pij_obs
#Below: observe that u_try is not equal to u_obs_guess - i.e., when there's a boundary (as here), can't do the naive deconvolution this way
#u_try <- pseudo_inv_pij_obs%*%d_obs
#u_try

q_j <- colSums(p_ij_obs)
#q_j
dim_p_ij <- dim(p_ij_obs)
p_ij_obs_rescaled <- p_ij_obs / matrix(q_j,nrow=dim_p_ij[1],ncol=dim_p_ij[2],byrow=TRUE)
#p_ij_obs_rescaled
u_obs_guess_rescaled <- u_obs_guess * q_j
#u_obs_guess_rescaled
d_obs_rescaled <- d_obs * q_j[(length(p)):length(u_obs_guess_rescaled)]

get_chi_sq <- function (vec1, vec2) {
  n0 <- length(vec1)
  (1/n0)*sum(((vec1 - vec2)^2/vec1))
}

u_obs_rescaled <- u_obs_guess_rescaled
chi_sq <- get_chi_sq(d_obs_rescaled, u_obs_rescaled[length(p):length(u_obs_rescaled)])
#u_obs_rescaled
#chi_sq
ind <- 1
parlist$max_RL_it
#chi_sq
#while (chi_sq > 1 & ind < parlist$max_RL_it) {
while (ind < parlist$max_RL_it) {
  c_obs <-p_ij_obs_rescaled %*% u_obs_rescaled
  #print(c_obs)
  new_kernel_obs <- d_obs/c_obs
  #print(t(new_kernel_obs)%*%p_ij_obs_rescaled)
  #print(u_obs_rescaled)
  new_u_obs_rescaled <- u_obs_rescaled * t(t(new_kernel_obs) %*% p_ij_obs_rescaled)
  #print(new_u)
  u_obs_rescaled <- new_u_obs_rescaled
  chi_sq <- get_chi_sq(d_obs_rescaled, u_obs_rescaled[length(p):length(u_obs_rescaled)])
  ind <- ind+1
}
 #ind 
#u_obs_rescaled
u_obs_new <- u_obs_rescaled/q_j
#u_obs_new
#plot((obs_1 - length(p) + 1):obs_2, u_obs_new, ylim = c(0, 60), xlim = c(0, 200), type = 'l') #observe: u_obs_new is essentially a precise match to u_true where they are both defined. just have to define the time right as compared to d.
#lines(1:length(u_true), u_true, col = 'red') #this is longer than u_obs_new because it's all of u, not just the subset gotten from "observed"
#points((obs_1 - length(p) + 1):obs_2, u_obs_guess, col = 'blue')
#lines(1:length(d), d, col = 'green') # this is what we observe - should be shifted from infections

#obs_1
```

```{r}
shift_val <- ceiling(median(misspec_obs_delay_dist(10000000)))
shifted_obs_cases <- sim_df$new_observed[shift_val:length(sim_df$new_observed)]

inferred_df <- data.frame(c(obs_1 - length(p) + 1):obs_2, u_obs_new, shifted_obs_cases[(obs_1-length(p)+1):obs_2])
names(inferred_df) <- c('time', "new_inferred", 'shifted_obs_cases')
#print(inferred_df)

#Hack for fixing indexing of time variable to be consistent between dataframes
inferred_df$time <- inferred_df$time - 2
sim_df$time <- sim_df$time # + 2

sim_df %>%
  merge(inferred_df, by = 'time', all = 'TRUE')    %>%
  as.tbl() %>%
  pivot_longer(-time, names_to = "data_type", values_to = "count") %>% 
  filter(data_type == "new_inferred"| data_type == "new_infected" | data_type == "new_observed" | data_type == "shifted_obs_cases") %>%
  ggplot() +
  geom_line(aes(x = time, y = count, color = data_type, linetype = data_type)) + 
  geom_vline(aes(xintercept = parlist$intervention_time_1), linetype = "dotted") +
  scale_linetype_manual(values=c("solid", "solid", "dotted", "solid")) + 
  scale_color_manual(values = c("blue", "red", "green", "green")) +
  labs(color = "Infections", linetype = "Infections") + 
  ylab("count")+
  xlab("time (days)") + 
  ggtitle(sprintf('Number of inferred cases from Richardson-Lucy')) -> inferred_plot

inferred_plot

saveRDS(inferred_plot, sprintf("%s/all-case-curves.Rds", intervention_file_name))
ggsave(sprintf("%s/all-case-curves.png", intervention_file_name))

```

```{r}
#diff <-  u_obs_new - u_true[(obs_1 - length(p) + 1):obs_2]

diff <-  u_obs_new - u_true[(obs_1 - length(p)):(obs_2-1)]
diff_shift <- inferred_df$shifted_obs_cases - u_true[(obs_1 - length(p)):(obs_2-1)]

sim_df2 <- sim_df
diff_df <- data.frame(c(obs_1 - length(p) + 1):obs_2, diff, diff_shift)
names(diff_df) <- c('time', "diff", "diff_shift")

sim_df2 %>%
  merge(diff_df, by = 'time', all = 'TRUE') %>%
  as.tbl()  %>%
  pivot_longer(-time, names_to = "data_type", values_to = "count") %>%
  filter(data_type == "diff" | data_type == "diff_shift" & !is.na(count)) %>%
  ggplot() +
  geom_line(aes(x = time, y = count, color = data_type, linetype = data_type)) + 
  geom_hline(aes(yintercept = 0, color = 'zero', linetype = 'zero'))+
  geom_vline(aes(xintercept = parlist$intervention_time_1, color = 'intervention', linetype = 'intervention')) +
  scale_linetype_manual(values=c("solid", "solid", "dotted", "solid")) + 
  scale_color_manual(values = c("red", "green", "black", "black")) +
 # scale_color_discrete(name = "Legend", labels = c("Difference = 0", "Time of intervention", "Infections, inferred minus true")) +
  labs(color = "Legend", linetype = "Legend") +
  ylab("count (difference)")+
  xlab("time (days)") + 
  ggtitle('Difference in number of inferred cases from Richardson-Lucy') -> diff_plot

diff_plot

saveRDS(diff_plot, sprintf("%s/difference-cases-RL.Rds", intervention_file_name))
ggsave(sprintf("%s/difference-cases-RL.png", intervention_file_name))

```
```{r}
#diff <-  u_obs_new - u_true[(obs_1 - length(p) + 1):obs_2]

diff_frac <-  abs((u_obs_new - u_true[(obs_1 - length(p)):(obs_2-1)])/(u_true[(obs_1 - length(p)):(obs_2-1)]))
diff_frac_shift <- abs((u_true[(obs_1 - length(p)):(obs_2-1)] - shifted_obs_cases[(obs_1 - length(p)):(obs_2-1)])/(u_true[(obs_1 - length(p)):(obs_2-1)]))

#diff_percent

sim_df3 <- sim_df
diff_df_frac <- data.frame(c(obs_1 - length(p) + 1):obs_2, diff_frac, diff_frac_shift)
names(diff_df_frac) <- c('time', "diff_frac", "diff_frac_shift")

sim_df3 %>%
  merge(diff_df_frac, by = 'time', all = 'TRUE') %>%
  as.tbl()  %>%
  pivot_longer(-time, names_to = "data_type", values_to = "count") %>%
  filter(data_type == "diff_frac" | data_type == "diff_frac_shift" & !is.na(count)) %>%
  ggplot() +
  geom_line(aes(x = time, y = count, color = data_type, linetype = data_type)) + 
  geom_hline(aes(yintercept = 0, color = 'zero', linetype = 'zero'))+
  geom_vline(aes(xintercept = parlist$intervention_time_1, color = 'intervention', linetype = 'intervention')) +
  scale_linetype_manual(values=c("solid", "solid", "dotted", "solid")) + 
  scale_color_manual(values = c("red", "green", "black", "black")) +
 # scale_color_discrete(name = "Legend", labels = c("Difference = 0", "Time of intervention", "Infections, inferred minus true")) +
  labs(color = "Legend", linetype = "Legend") +
  ylab("count (abs((inferred-true)/true))")+
  xlab("time (days)") + 
  ggtitle('Difference ratio: abs((inferred-true)/true)') -> diff_frac_plot

diff_frac_plot

saveRDS(diff_frac_plot, sprintf("%s/difference-frac-cases-RL.Rds", intervention_file_name))
ggsave(sprintf("%s/difference-frac-cases-RL.png", intervention_file_name))

```



```{r}

plot_grid(inferred_plot, diff_plot, diff_frac_plot, labels = "AUTO", ncol = 1, align = 'v') -> all_plot

all_plot

saveRDS(all_plot, sprintf("%s/all_plot.Rds", intervention_file_name))
ggsave(sprintf("%s/all_plot.png", intervention_file_name))
```

##Carry out MCMC on the simulated data to gt error estimates on the deconvolution. Reference: Sampling-based uncertainty quantification in deconvolution of X-ray radiographs. Howard, Luttman and Fowler, 2013

Initializing necessary quantities. To agree with notation from the original paper, $\vec{b}$ is the vector of observed counts, $\vec{u}$ is a vector of true counts such that $p(\vec{u}\vert\vec{b})$ is the probability that the true counts were $\vec{u}$ given that we observed $\vec{b}$. The prior $p(\vec{u})$ depends on a hyperprior which we call $\lambda$. The MCMC samples the distributions $p(\vec{u}\vert \lambda,\vec{b})$ and $p(\lambda \vert \vec{u}, \vec{b})$.
```{r}
#for MCMC error estimation
library(expm)
library(matlib)
library(MASS)

conv_matrix <- p_ij_obs #toeplitz matrix for carrying out convolution was defined in RL section of code
true_dat <-  u_obs_new  #this initializes the MCMC - give it the RL inferred infections since this is our best guess.
obs_dat <- d_obs #only want the observed data within the chosen time period. see the RL section for original definition. #sim_df$new_observed.
  
backgrd <- 1 #could add nontrivial background counts. nonzero to normalize log in posterior. integer because all counts must be integers.

#hyperprior for lambda: gamma, mean is alpha/beta and variance is alpha/beta^2
beta <- 10^0
alpha <- 1

dt <- 1 #unit of time steps, in days

max_N <- 3000
burn_in <- 1000
lambda0 <- alpha/beta #start with initial value of lambda = mean of hyperprior
```

Total variation (edge-enhancing) prior for $\vec{u}$ requires this form of the covariance matrix. NOTE: This function is slightly "wrong". The $\vec{u}$ in the definition of $L$ should be $u_{\lambda}$ - i.e., $\vec{u}$ solved from RL with regularization on $\grad{u}$. For now, try it this way, but eventually, fix.

```{r}
diffmat <- function(len) {
  mat <- matrix(0, nrow = len, ncol = len)
  for(i in c(1:len)){
    if(i==1) {
      mat[i, i] <- -1/dt
      mat[i, i+1] <- 1/dt
    } else if(i==len){
      mat[i, i-1] <- -1/dt
      mat[i, i] <- 1/dt
    } else {
      mat[i, i+1] <- 1/(2*dt)
      mat[i, i-1] <- -1/(2*dt)
    }
  }
  mat
}

#NOTE: CHANGED LTV TO BE COMPARING TO true_dat, WHICH IS LIKE u_lambda WHEN lambda = 0, i.e. no constraint on smoothness
LTV <- function(u, beta){
  u <- true_dat
  Dx <- diffmat(length(u))
  phi <- 1/(sum((Dx %*% u)^2) + beta)
  phimat <- phi * diag(length(u))
  t(Dx) %*% phimat %*% Dx  
}
```

Define the conditional distributions that we will actually sample from, as derived from the previous conditional probabilities (see reference).
```{r}
#TO DO: Add a strong penalty -h*u/|u| for negative values of u, or something

get_u_mean <- function (A, b, u, lambda, beta) {
  C <- diag(b+1)
  inv_mean_1 <- t(A) %*% solve(C) %*% A + lambda * LTV(u, beta)
  mean_1 <- solve(inv_mean_1)
  mean_2 <- t(A) %*% solve(C) %*% b
  mean_1 %*% mean_2
}

get_u_covar <- function (A, b, u, lambda, beta) {
  C <- diag(b+1)
  solve(t(A) %*% solve(C) %*% A + lambda * LTV(u, beta))
}

sample_u_given_lambda_b <- function (A, b, u, lambda, beta, alpha) {
  abs(mvrnorm(n = 1, get_u_mean(A, b, u, lambda, beta), get_u_covar(A, b, u, lambda, beta)))
}

sample_lambda_given_u_b <- function (A, b, u, lambda, beta, alpha) {
  rgamma(1, length(u)/2 + alpha, 1/2 * t(u) %*% LTV(u, beta) %*% u + beta)
}
```


```{r}
lambda <- lambda0
curr_u <- true_dat #initializing MCMC with the RL-inferred incidence curve at time of infection.

all_u <- matrix(, nrow = max_N, ncol = length(true_dat)) #samples of the $p(\vec{u}\vert \vec{b})$ distribution (with $\lambda$?)
all_lambda <- matrix(, nrow = max_N, ncol = 1) #samples the $\lambda$ distribution.

all_u[1,] <- curr_u
all_lambda[1,] <- lambda

max_N = 500

for (k in c(2:max_N)) {
  new_u <-  sample_u_given_lambda_b(conv_matrix, obs_dat, curr_u, lambda, beta)
  new_lambda <- sample_lambda_given_u_b(conv_matrix, obs_dat, curr_u, lambda, beta, alpha)
  
  all_u[k,] <- new_u
  all_lambda[k,] <- new_lambda
  
  curr_u <- new_u
  lambda <- new_lambda
}
```

```{r}
dim(all_u)
plot(all_u[1000, ])
plot(colMeans(all_u[500:1000, ]))
lines(inferred_df$new_inferred, col = 'blue')
lines(sim_df$new_infected, col = 'red')
plot(all_lambda)
mean(all_lambda[500:1000,])

# sim_df %>%
#   merge(inferred_df, by = 'time', all = 'TRUE') %>%
#   as.tbl()  %>%
#   pivot_longer(-time, names_to = "data_type", values_to = "count") %>% 
#   filter(data_type == "new_inferred"| data_type == "new_infected" | data_type == "new_observed") %>%
#   ggplot() +
#   geom_line(aes(x = time, y = count, color = data_type, linetype = data_type)) + 
#   geom_vline(aes(xintercept = parlist$intervention_time_1), linetype = "dotted") +
#   scale_linetype_manual(values=c("solid", "solid", "dotted")) + 
#   scale_color_manual(values = c("blue", "red", "green")) +
#   labs(color = "Infections", linetype = "Infections") + 
#   ylab("count")+
#   xlab("time (days)") + 
#   ggtitle('Number of inferred cases from Richardson-Lucy') -> inferred_plot_with_MCMC
# 
# inferred_plot_with_MCMC
# 
# saveRDS(inferred_plot_with_MCMC, sprintf("%s/all-case-curves-with-error.Rds", intervention_file_name))
# ggsave(sprintf("%s/all-case-curves-with-error.png", intervention_file_name))

```


